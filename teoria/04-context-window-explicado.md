# Context Window Explicado: A Mem√≥ria Limitada da IA

## üéØ O que voc√™ vai aprender

Context window √© o **limite de mem√≥ria** de um LLM ‚Äî quantos tokens ele pode "lembrar" de uma vez. Entender isso √© crucial para intera√ß√µes eficazes, especialmente em conversas longas ou an√°lises de documentos extensos.

## üß† Por que isso importa?

- **Evita frustra√ß√µes**: "Por que a IA esqueceu o que eu disse?"
- **Otimiza custos**: Tokens custam dinheiro em APIs
- **Melhora estrat√©gia**: Saber quando dividir tarefas
- **Entende limita√ß√µes**: IA n√£o tem mem√≥ria de longo prazo real

**Analogia cognitiva**: Context window √© como **mem√≥ria de trabalho** (working memory) humana ‚Äî processa 5-9 itens simult√¢neos, depois "esquece".

---

## üìñ Explica√ß√£o

### O que √© Context Window?

O **n√∫mero m√°ximo de tokens** (palavras + fragmentos) que um LLM pode processar de uma vez, incluindo:

1. **Prompt de sistema** (instru√ß√µes para a IA)
2. **Hist√≥rico de conversa** (mensagens anteriores)
3. **Seu prompt atual** (nova pergunta)
4. **Resposta gerada** (output da IA)

**Matem√°tica simples**:
```
Context Window Total = Prompt Sistema + Hist√≥rico + Prompt Atual + Resposta
```

---

### Tamanhos de Context Window (2024)

| Modelo | Context Window | Equivalente em palavras |
|--------|----------------|-------------------------|
| GPT-3.5 Turbo | 16k tokens | ~12.000 palavras |
| GPT-4 | 8k / 32k tokens | ~6k / ~24k palavras |
| GPT-4 Turbo | 128k tokens | ~96.000 palavras |
| Claude 3 Opus | 200k tokens | ~150.000 palavras |
| Claude 3.5 Sonnet | 200k tokens | ~150.000 palavras |
| Gemini 1.5 Pro | 1M tokens | ~750.000 palavras |

**Refer√™ncia**: 1 token ‚âà 0,75 palavras em portugu√™s

---

## üîÑ O que Acontece Quando o Limite √© Atingido?

### Estrat√©gias de Diferentes Modelos

#### 1. **Truncamento (Corte)**
```
Conversa: [msg 1, msg 2, msg 3, ..., msg 50]
Context cheio ‚Üí Remove msg 1, msg 2...
Mant√©m: [msg 40, msg 41, ..., msg 50]
```
‚úÖ Simples  
‚ùå Perde contexto importante do in√≠cio

#### 2. **Sumariza√ß√£o Autom√°tica**
```
Conversa: [hist√≥rico longo]
Context cheio ‚Üí Gera resumo do in√≠cio
Mant√©m: [resumo] + [mensagens recentes]
```
‚úÖ Preserva informa√ß√£o essencial  
‚ö†Ô∏è Pode perder nuances

#### 3. **Erro e Recusa**
```
Context cheio ‚Üí "Erro: Token limit exceeded"
```
‚ùå Interrompe fluxo

---

## üß† Psicologia do Context Window

### Paralelo com Mem√≥ria Humana

| Mem√≥ria Humana | LLM Context Window |
|----------------|---------------------|
| **Mem√≥ria de Trabalho**: 5-9 itens | Context window ativo |
| **Mem√≥ria de Longo Prazo**: Ilimitada | ‚ùå N√£o existe em LLMs |
| **Chunking**: Agrupar info para lembrar mais | T√©cnicas de prompt design |
| **Rehearsal**: Repetir para n√£o esquecer | Re-injetar contexto no prompt |

**Diferen√ßa cr√≠tica**: Humanos transferem mem√≥ria de trabalho para longo prazo. LLMs **n√£o**.

---

## üõ†Ô∏è Estrat√©gias para Gerenciar Context Window

### 1. **Chunking (Divis√£o em Partes)**

**Problema**: Documento de 200 p√°ginas para analisar

**Solu√ß√£o**:
```python
# Divide em chunks de 10 p√°ginas
for i in range(20):
    chunk = documento[i*10:(i+1)*10]
    resposta = llm.analyze(chunk)
    resumos.append(resposta)

# Depois analisa resumos agregados
analise_final = llm.synthesize(resumos)
```

---

### 2. **T√©cnica do Resumo Progressivo**

**Conversa longa**:
```
Turno 1-10: Discuss√£o sobre ansiedade
Turno 11-20: T√©cnicas de respira√ß√£o
Turno 21-30: Exerc√≠cios pr√°ticos
```

**A cada 10 turnos**:
```
Prompt: "Resuma nossa conversa at√© agora em 3 frases"
[Salva resumo]
[Continua com resumo + novas mensagens]
```

---

### 3. **Prompts Autocontidos**

**‚ùå Dependente de contexto**:
```
Msg 1: "Analise este relat√≥rio [10k palavras]"
Msg 2: "Qual a conclus√£o sobre vendas?" 
```
‚Üí Se contexto for perdido, msg 2 falha

**‚úÖ Autocontido**:
```
Msg 1: "Analise este relat√≥rio [10k palavras]"
Msg 2: "No relat√≥rio sobre vendas Q3 2024 que enviei, 
        qual a conclus√£o sobre performance regional?"
```
‚Üí Funciona mesmo se contexto anterior for truncado

---

### 4. **Uso de Mem√≥ria Externa**

**Para desenvolvedores**:
```python
# Armazena contexto importante em banco de dados
memory_store = {
    "user_preferences": {...},
    "conversation_summary": "...",
    "key_facts": [...]
}

# Injeta seletivamente no prompt
prompt = f"""
Contexto relevante: {memory_store['key_facts']}
Nova pergunta: {user_input}
"""
```

---

## üîç Exemplo Pr√°tico: Gest√£o de Context Window

### Cen√°rio: Revis√£o de TCC

**Documento**: 80 p√°ginas (‚âà60k palavras = ~80k tokens)  
**Context window**: 32k tokens

**‚ùå Abordagem Ing√™nua**:
```
"Leia meu TCC [cola 80 p√°ginas] e me d√™ feedback"
```
Resultado: Erro ou truncamento severo

**‚úÖ Abordagem Estrat√©gica**:

```python
# Etapa 1: Resumo por cap√≠tulo
for capitulo in tcc.capitulos:
    resumo = llm.resumir(capitulo, max_tokens=500)
    resumos.append(resumo)

# Etapa 2: An√°lise estrutural
prompt = f"""
Resumos dos cap√≠tulos:
{resumos}

Analise:
1. Coer√™ncia entre cap√≠tulos
2. Gaps argumentativos
3. Sugest√µes de melhoria
"""

# Etapa 3: Aprofundamento seletivo
"Agora leia apenas o Cap√≠tulo 3 [texto completo] e critique metodologia"
```

---

## ü§î Quest√µes para Reflex√£o

1. **Se LLMs tivessem mem√≥ria infinita, isso os tornaria "conscientes"?** Ou mem√≥ria √© apenas parte da equa√ß√£o?

2. **Context window limitado √© bug ou feature?** For√ßa usu√°rios a serem concisos?

3. **Para psic√≥logos**: Limite de mem√≥ria humana √© evolutivamente adaptativo. E para IAs?

4. **Modelos com 1M tokens (Gemini) s√£o "melhores"?** Ou h√° tradeoffs de qualidade?

5. **Se voc√™ pudesse "congelar" partes do contexto (nunca esquecer), o que voc√™ congelaria?**

---

## üß™ Experimentos

### Experimento 1: Teste de Limite
```
1. Inicie conversa
2. A cada turno, pergunte: "Voc√™ se lembra do que eu disse na mensagem 1?"
3. Continue at√© a IA esquecer
4. Conte quantos turnos foram necess√°rios
```

### Experimento 2: Resumo vs. Texto Completo
```
A) Envie documento longo pedindo an√°lise completa
B) Envie resumo do documento pedindo an√°lise

Compare qualidade das respostas
```

### Experimento 3: Chunking Eficaz
```
Divida texto em:
A) Chunks aleat√≥rios de 1000 tokens
B) Chunks sem√¢nticos (por se√ß√£o/t√≥pico)

Qual preserva mais coer√™ncia?
```

---

## üìö Refer√™ncias

### Papers
- **"Attention Is All You Need"** ‚Äì Vaswani et al. (2017) [Base do Transformer]
- **"Long Range Arena"** ‚Äì Tay et al. (2020) [Desafios de longo contexto]
- **"Lost in the Middle"** ‚Äì Liu et al. (2023) [LLMs esquecem info no meio do contexto]

### Recursos T√©cnicos
- **OpenAI Token Counting**: [OpenAI Tokenizer](https://platform.openai.com/tokenizer)
- **LangChain Memory**: [Docs sobre gerenciamento de mem√≥ria](https://python.langchain.com/docs/modules/memory/)

### Artigos
- **"Understanding Context Windows"** ‚Äì Anthropic Blog
- **"Effective Context Management"** ‚Äì OpenAI Cookbook

---

## ‚û°Ô∏è Pr√≥ximos Passos

1. **[Como Funcionam os LLMs](como-funcionam-os-llms.md)** ‚Üí Entenda por que contexto √© limitado
2. **[Tipos de Prompting](tipos-de-prompting.md)** ‚Üí T√©cnicas para lidar com contexto longo
3. **[RAG (Retrieval Augmented Generation)](rag-retrieval-augmented-generation.md)** ‚Üí Superar limita√ß√µes de contexto

---

## üéì Nota do Autor

Context window √© a **mem√≥ria de trabalho da IA**. Assim como humanos precisam de anota√ß√µes para lembrar de informa√ß√µes complexas, LLMs precisam de estrat√©gias de gest√£o de contexto.

A limita√ß√£o n√£o √© fraqueza ‚Äî √© caracter√≠stica. Aprenda a trabalhar com ela, n√£o contra ela.

Como psic√≥logo, voc√™ entende que limita√ß√µes cognitivas moldam como pensamos. O mesmo vale para IA. Context window define **como** interagimos com ela.

---

**Escrito por Gabriel, Arquiteto Cognitivo**  
*√öltima atualiza√ß√£o: Dezembro 2024*
