# Papers e Artigos Importantes - IA Generativa

Guia curado de pesquisas fundamentais que moldaram o campo de IA Generativa. Organizado por t√≥pico e impacto.

---

## üéØ Funda√ß√µes (Leitura Essencial)

### 1. **"Attention Is All You Need"** (2017)
**Autores**: Vaswani et al.  
**Publicado em**: Proceedings of NeurIPS  
**Link**: https://arxiv.org/abs/1706.03762

**Por qu√™ ler**: Define a arquitetura Transformer que √© base de TODOS os LLMs modernos. Altamente impactante.

**Contexto**: Antes disso, RNNs dominavam. Este paper mostrou que voc√™ pode fazer tudo melhor com apenas aten√ß√£o.

**Dificuldade**: M√©dia-Alta. Requer conhecimento de redes neurais, mas √© bem escrito.

---

### 2. **"Language Models are Unsupervised Multitask Learners"** (2019)
**Autores**: Radford et al. (OpenAI)  
**Conhecido como**: GPT-2 Paper  
**Link**: https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf

**Por qu√™ ler**: Demonstra que modelos grandes conseguem fazer m√∫ltiplas tarefas sem treinamento espec√≠fico (in-context learning).

**Contexto**: Um dos primeiros sinais de que aumentar escala gera capacidades emergentes.

**Dificuldade**: Baixa. Mais acess√≠vel que papers anteriores.

---

### 3. **"Language Models are Few-Shot Learners"** (2020)
**Autores**: Brown et al. (OpenAI)  
**Conhecido como**: GPT-3 Paper  
**Link**: https://arxiv.org/abs/2005.14165

**Por qu√™ ler**: Mostrou que few-shot learning em larga escala √© vi√°vel. Fundamenta√ß√£o te√≥rica de como LLMs funcionam.

**Contexto**: GPT-3 tem 175B par√¢metros. Primeiro grande modelo que mostrou verdadeiras capacidades conversacionais.

**Dificuldade**: M√©dia. Longo mas bem organizado.

---

## üî¨ Mecanismos e T√©cnicas

### 4. **"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** (2018)
**Autores**: Devlin et al. (Google)  
**Link**: https://arxiv.org/abs/1810.04805

**Por qu√™ ler**: Mostrou treinamento bidirecional em larga escala. Importante para entender diferentes abordagens de treinamento.

**Contexto**: BERT foi revolucion√°rio na √©poca, embora GPT-style (unidirecional) tenha prevalecido para gera√ß√£o.

**Dificuldade**: M√©dia.

---

### 5. **"Scaling Laws for Neural Language Models"** (2020)
**Autores**: Kaplan et al. (OpenAI)  
**Link**: https://arxiv.org/abs/2001.08361

**Por qu√™ ler**: Estabelece padr√µes matem√°ticos de como performance melhora com tamanho de modelo, dados e computa√ß√£o.

**Contexto**: Explica por que "mais grande = melhor" at√© certo ponto. Guiou decis√µes de design do GPT-3.

**Dificuldade**: M√©dia-Alta. Matem√°tica t√©cnica, mas insights claros.

---

### 6. **"Emergent Abilities of Large Language Models"** (2022)
**Autores**: Wei et al. (Google)  
**Link**: https://arxiv.org/abs/2206.07682

**Por qu√™ ler**: Define e explora "emerg√™ncia" - quando modelos mostram capacidades n√£o explicitamente treinadas.

**Contexto**: Por que um modelo de 7B pode ser "burro" mas 70B √© "inteligente"?

**Dificuldade**: M√©dia.

---

### 7. **"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"** (2022)
**Autores**: Wei et al. (Google)  
**Link**: https://arxiv.org/abs/2201.11903

**Por qu√™ ler**: Demonstra que pedir ao modelo para "pensar em voz alta" melhora significativamente precis√£o em tarefas complexas.

**Contexto**: Um dos papers mais impactantes de prompt engineering.

**Dificuldade**: Baixa. Muito acess√≠vel.

---

### 8. **"Prompt Engineering for Biomedical Named Entity Recognition via LLMs"** (2023)
**Autores**: Varios  
**Contexto**: Exemplo de como t√©cnicas de prompt engineering se aplicam a dom√≠nios espec√≠ficos.

---

## üß† Comportamento e Interpretabilidade

### 9. **"Mechanistic Interpretability for Large Language Models"** (2023)
**Autores**: Nanda et al.  
**Link**: https://www.anthropic.com/research

**Por qu√™ ler**: Como podemos entender o que LLMs est√£o fazendo internamente? Importante para seguran√ßa.

**Contexto**: Crescente foco em "interpretabilidade" - abrir a "caixa preta".

**Dificuldade**: Alta. Altamente t√©cnico.

---

### 10. **"Constitutional AI: Harmlessness from AI Feedback"** (2022)
**Autores**: Bai et al. (Anthropic)  
**Link**: https://arxiv.org/abs/2212.08073

**Por qu√™ ler**: Abordagem inovadora para fazer LLMs mais seguros e alinhados com valores humanos.

**Contexto**: Como treinar modelos de forma √©tica?

**Dificuldade**: M√©dia.

---

## üé® Modelos Multimodais

### 11. **"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"** (2020)
**Autores**: Dosovitskiy et al.  
**Conhecido como**: Vision Transformer (ViT)  
**Link**: https://arxiv.org/abs/2010.11929

**Por qu√™ ler**: Mostra como Transformers funcionam n√£o apenas com texto mas com imagens tamb√©m.

**Contexto**: Base te√≥rica para modelos multimodais modernos.

**Dificuldade**: M√©dia.

---

### 12. **"Learning Transferable Visual Models From Natural Language Supervision"** (2021)
**Autores**: Radford et al. (OpenAI)  
**Conhecido como**: CLIP  
**Link**: https://arxiv.org/abs/2103.14030

**Por qu√™ ler**: CLIP √© revolucion√°rio para conectar vis√£o e linguagem. Base de muitos sistemas multimodais.

**Contexto**: Como treinar um modelo com imagens + textos juntos?

**Dificuldade**: M√©dia.

---

## üíæ Efici√™ncia e Otimiza√ß√£o

### 13. **"LoRA: Low-Rank Adaptation of Large Language Models"** (2021)
**Autores**: Hu et al.  
**Link**: https://arxiv.org/abs/2106.09685

**Por qu√™ ler**: T√©cnica revolucion√°ria que permite fine-tuning de modelos gigantes com fra√ß√£o do custo.

**Contexto**: Por qu√™ usar LoRA em vez de fine-tuning completo?

**Dificuldade**: M√©dia.

---

### 14. **"Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference"** (2018)
**Autores**: Jacob et al. (Google)  
**Link**: https://arxiv.org/abs/1806.08342

**Por qu√™ ler**: Como comprimir modelos sem perder muita performance.

**Contexto**: Importante para rodar LLMs em dispositivos m√≥veis.

**Dificuldade**: Alta. T√©cnico.

---

## üîê Seguran√ßa e Alinhamento

### 15. **"Towards a Unified Theory of Deep Undermining in Language Models"** (2023+)
**Contexto**: Crescente pesquisa em adversarial attacks contra LLMs.

---

### 16. **"Language Models Can Explain Themselves"** (2023)
**Autores**: Varios  
**Contexto**: LLMs podem explicar por que deram uma resposta? Importante para auditoria.

---

## üìä Avalia√ß√£o e Benchmarks

### 17. **"MMLU: A Massive Multitask Language Understanding"** (2020)
**Autores**: Hendrycks et al.  
**Link**: https://arxiv.org/abs/2009.03300

**Por qu√™ ler**: Define benchmark padr√£o para avaliar LLMs em m√∫ltiplas disciplinas.

**Contexto**: Como comparar quantitativamente a "intelig√™ncia" de modelos?

**Dificuldade**: Baixa. Mais um dataset que paper te√≥rico.

---

### 18. **"BIG-bench: A Benchmark for Language Models"** (2023)
**Autores**: Suzgun et al. (Google)  
**Link**: https://arxiv.org/abs/2301.12873

**Por qu√™ ler**: Benchmark inclusivo com 200+ tarefas. Bom para ver range completo de capacidades.

**Contexto**: Mais abrangente que MMLU.

**Dificuldade**: Baixa.

---

## üåç Aplica√ß√µes Pr√°ticas

### 19. **"GPT-3.5 Technical Report"** (ChatGPT Release)
**Dispon√≠vel**: OpenAI blog

**Por qu√™ ler**: Documenta diferen√ßas entre GPT-3 e GPT-3.5 (que rodeia ChatGPT).

---

### 20. **"Sparks of Artificial General Intelligence: Early experiments with GPT-4"** (2023)
**Autores**: Bubeck et al. (Microsoft)  
**Link**: https://arxiv.org/abs/2303.12712

**Por qu√™ ler**: An√°lise aprofundada de capacidades emergentes do GPT-4. Especulativo mas insightful.

**Contexto**: Estamos perto de AGI? Que capacidades faltam?

**Dificuldade**: M√©dia. Mais especulativo que t√©cnico.

---

## üîÆ Pesquisa Atual (Fronteira)

### T√≥picos em Evolu√ß√£o R√°pida (2024-2025):

- **Mixture of Experts (MoE)**: Modelos que n√£o ativam todos os par√¢metros. Mais eficientes.
- **Multimodal Alignment**: Conectar texto, imagem, √°udio melhor.
- **Reasoning and Planning**: LLMs melhores em tarefas multi-step complexas.
- **Efficient Transformers**: Varia√ß√µes com menos complexidade computacional.
- **Retrieval-Augmented Generation (RAG)**: Combinar busca com gera√ß√£o.
- **Agent Systems**: LLMs como agentes aut√¥nomos que planejam e executam.

---

## üìö Recursos para Encontrar Papers

- **ArXiv**: https://arxiv.org/ (reposit√≥rio principal de papers)
- **OpenReview**: https://openreview.net/ (confer√™ncias como NeurIPS, ICLR)
- **Anthropic Research**: https://www.anthropic.com/research
- **Google AI Blog**: https://ai.googleblog.com/
- **OpenAI Research**: https://openai.com/research/
- **Papers with Code**: https://paperswithcode.com/ (papers + implementa√ß√µes)

---

## üéì Como Ler Papers Acad√™micos

1. **Come√ßar pelo abstract e conclus√£o** - Entenda o objetivo e resultado
2. **Depois ler introdu√ß√£o** - Contexto e motiva√ß√£o
3. **Pular para figuras/gr√°ficos** - Insights visuais
4. **Ler se√ß√µes relevantes** - Nem sempre precisa ler tudo
5. **Ignorar math densa** - Se n√£o entender uma equa√ß√£o, n√£o √© o fim do mundo
6. **Procurar no Google** - Se n√£o entender algo, provavelmente algu√©m explicou em blog

**Dica de um psic√≥logo**: Reler papers em tempos diferentes ajuda (spaced repetition). Voc√™ vai entender mais a cada passada!

---

## ü§ù Discuss√µes e Comunidades

- **r/MachineLearning**: Reddit com discuss√µes t√©cnicas
- **Papers with Code Discussions**: Comunidade comentando papers
- **Twitter/X Machine Learning**: Pesquisadores compartilhando insights
- **Hugging Face Forums**: Comunidade pr√°tica
- **Anthropic/OpenAI Communities**: Comunidades oficiais

---

**√öltima atualiza√ß√£o**: Dezembro 2024

Este documento est√° em evolu√ß√£o. Novos papers fundamentais surgem frequentemente!
